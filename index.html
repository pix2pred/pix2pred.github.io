<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <base target="_blank">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="assets/tetris-icon.png">
    <title>From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <!-- Set initial theme and update images before first paint to prevent flashing or mismatch images -->
    <script>
        (function () {
          const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
          document.documentElement.setAttribute('data-theme', prefersDark ? 'dark' : 'light');
        })();

        (function () {
            const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
            document.documentElement.setAttribute('data-theme', prefersDark ? 'dark' : 'light');

            // Update dark/light images immediately
            window.addEventListener('DOMContentLoaded', () => {
            document.querySelectorAll('img[data-dark-src]').forEach(img => {
                const desiredSrc = img.getAttribute(prefersDark ? 'data-dark-src' : 'data-light-src');
                img.src = desiredSrc;
            });
            });
        })();
    </script>

    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          }
        };
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/js/all.min.js"></script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-291MD67PHF"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-291MD67PHF');
    </script>
</head>
<body>
    <section class="section">
        <div class="container">
            <button class="button dark-mode-toggle is-medium" onclick="toggleDarkMode()">
                <span class="icon">
                    <i class="fas fa-moon" id="theme-icon"></i>
                </span>
            </button>
            <div class="content">
                <div class="toc-container">
                    <div class="toc-content">
                        <aside class="menu">
                            <p class="menu-label">
                                Table of Contents
                            </p>
                            <ul class="menu-list">
                                <li><a href="#method" target="_self">Method</a></li>
                                <li><a href="#tetris" target="_self">Solving Highly-Constrained Problems</a></li>
                                <li><a href="#goal-costs" target="_self">Optimizing Goal Costs</a></li>
                                <li><a href="#skeleton-search" target="_self">Efficiently Searching over Plan Skeletons</a></li>
                            </ul>
                        </aside>
                    </div>
                </div>

                <h1 class="title is-1 is-size-2-mobile mt-0">From Pixels to Predicates:<br>Learning Symbolic World Models via Pretrained Vision-Language Models</h1>

                <div class="author-info">
                    <p class="subtitle">
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://ashay.io/">Ashay Athalye</a><sup>*,1,4</sup>,</div>
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://nishanthjkumar.com">Nishanth Kumar</a><sup>*,1</sup>,</div>
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://tsilver.com">Tom Silver</a><sup>2</sup>,</div>
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://yichao-liang.github.io/">Yichao Liang</a><sup>3</sup>,</div>
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://www.robo.guru/">Jiuguang Wang</a><sup>4</sup>,</div>
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://people.csail.mit.edu/tlp/">Tom√°s Lozano-P√©rez</a><sup>1</sup>,</div>
                        <div class="is-inline-block is-size-5"><a class="has-text-weight-semibold" href="https://people.csail.mit.edu/lpk/">Leslie Pack Kaelbling</a><sup>1</sup></div>
                    </p>
                    <p>
                        <div class="is-inline-block"><sup>1</sup><a href="https://www.csail.mit.edu/">MIT CSAIL</a>,</div>
                        <div class="is-inline-block"><sup>2</sup><a href="https://www.cs.princeton.edu/">Princeton University</a>,</div>
                        <div class="is-inline-block"><sup>3</sup><a href="https://www.cam.ac.uk/">University of Cambridge</a>,</div>
                        <div class="is-inline-block"><sup>4</sup><a href="https://rai-inst.com/">Robotics and AI Institute</a></div>
                    </p>
                    <p class="is-size-7">*Equal contribution</p>
                </div>                

                <div class="columns is-vcentered mt-1">
                    <div class="column is-narrow">
                        <!-- <a href="cutamp.pdf" class="button is-size-5 mr-1 mt-2">
                            <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>Paper</span>
                        </a> -->

                        <a href="https://arxiv.org/abs/2411.11833" class="button is-size-5 mr-1 mt-2">
                            <span class="icon">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            <span>arXiv</span>
                        </a>

                        <a class="button is-size-5 mr-1 mt-2" disabled>
                            <span class="icon">
                                <i class="fas fa-code"></i>
                            </span>
                            <span>Code (coming soon)</span>
                        </a>
                    </div>
                </div>
                
                <div class="video-card">
                    <video autoplay controls loop muted="muted" muted playsinline alt="Teaser Video">
                        <source src="assets/videos/teaser.mp4" type="video/mp4">
                    </video>
                </div>

                <h2 id="abstract" class="title is-4 mb-2">Abstract</h2>
                <p style="font-size: 0.95em">
                    Our aim is to learn to solve long-horizon decision-making problems in complex robotics domains given low-level skills and a handful of short-horizon demonstrations containing sequences of images.
                    To this end, we focus on learning abstract symbolic world models that facilitate zero-shot generalization to novel goals via planning. 
                    A critical component of such models is the set of symbolic <em>predicates</em> that define properties of and relationships between objects.
                    In this work, we leverage pretrained vision-language models (VLMs) to propose a large set of visual predicates potentially relevant for decision-making, and to evaluate those predicates directly from camera images.
                    At training time, we pass the proposed predicates and demonstrations into an optimization-based model-learning algorithm to obtain an abstract symbolic world model that is defined in terms of a compact subset of the proposed predicates. 
                    At test time, given a novel goal in a novel setting, we use the VLM to construct a symbolic description of the current world state, and then use a search-based planning algorithm to find a sequence of low-level skills that achieves the goal. 
                    We demonstrate empirically across experiments in both simulation and the real world that our method can generalize aggressively, applying its learned world model to solve problems with a wide variety of object types, arrangements, numbers of objects, and visual backgrounds, as well as novel goals and much longer horizons than those seen at training time.
                </p>

                <hr class="divider">

                <h2 id="method" class="title is-3">Method</h2>
                <img src="assets/cutamp-pipeline.png" alt="cuTAMP Pipeline" data-dark-src="assets/cutamp-pipeline-dark.png" data-light-src="assets/cutamp-pipeline.png">

                <p class="is-size-6">
                    <b>pix2pred Overview.</b> cuTAMP frames TAMP as a backtracking bilevel search over plan skeletons. Each
                    skeleton $\pi$ induces a continuous Constraint Satisfaction Problem that defines the structure of a particle (parameters) and cost
                    functions (constraints and plan costs). These particles are optimized in parallel by evaluating their costs with differentiable
                    cost functions, allowing gradient-based optimizers to iteratively update them towards satisfying solutions.
                </p>

                <h2 id="tetris" class="title is-3">Solving Highly-Constrained Problems</h2>
                <p>
                    In the Tetris domain, the robot's objective is to pack 5 blocks with non-convex shapes somewhere in a tight goal region.
                    This problem requires reasoning about spatial arrangements, as the shapes will only fit if they are arranged in particular configuration modes.
                </p>
                <p>
                    We optimize for placement poses, parametrized as <b>continuous</b> 4-DOF actions with positions $(x, y, z)$ and yaw angles $\theta$, along with their associated 7-DOF robot joint positions.
                    Each particle is 90-dimensional (ten 7-DOF arm configurations and five 4-DOF placement poses).
                    We sample a fixed set of grasps and use an off-the-shelf motion planner (<a href="https://curobo.org/">cuRobo</a>) to solve for the full trajectories.
                </p>
                <div class="columns is-multiline">
                    <div class="column is-half">
                        <div class="video-card">
                            <video autoplay controls loop muted playsinline alt="Tetris Optimization">
                                <source src="assets/videos/tetris-optimization.mp4" type="video/mp4">
                            </video>
                            <div class="video-card-content">
                                <div class="video-title">Optimization Visualization</div>
                                <p>We visualize the placement poses corresponding to the best particle as optimization progress.</p>
                            </div>
                        </div>
                    </div>
                    <div class="column is-half">
                        <div class="video-card">
                            <video autoplay controls loop muted playsinline alt="Tetris Execution">
                                <source src="assets/videos/tetris-execution-2x.mp4" type="video/mp4">
                            </video>
                            <div class="video-card-content">
                                <div class="video-title">TAMP Plan Execution</div>
                                <p>
                                    We show the full execution of the TAMP plan for the best particle. cuTAMP optimizes for a solution in just seconds.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <h2 id="goal-costs" class="title is-3">Optimizing Goal Costs</h2>
                <p>
                    <b>cuTAMP</b> supports optimizing plan costs. We consider placing four objects into a large goal region and set a goal cost to minimize the distance between the objects in the final state.
                    Our results demonstrate the clear benefit of combining sampling with differentiable optimization for substantially reducing the goal cost.
                </p>
                <div class="columns is-multiline">
                    <div class="column is-half">
                        <div class="video-card">
                            <img src="assets/min-goal-dist.png" alt="Goal Cost Optimization" data-dark-src="assets/min-goal-dist-dark.png" data-light-src="assets/min-goal-dist.png">
                            <div class="video-card-content">
                                <div class="video-title">Minimizing Distance between Objects</div>
                                <p>
                                    We compare the final state after executing the best particle.
                                    (a) cuTAMP achieves significantly lower cost compared to (b) parallelized sampling which performs no optimization.
                                </p>
                            </div>
                        </div>
                    </div>
                    <div class="column is-half">
                        <div class="video-card">
                            <video autoplay controls loop muted playsinline alt="Minimizing y-position of objects">
                                <source src="assets/videos/rummy-min-y.mp4" type="video/mp4">
                            </video>
                            <div class="video-card-content">
                                <div class="video-title">Minimizing $y$-position of objects</div>
                                <p>
                                    We minimize the $y$-position of three cubes on a real Kinova Gen3 robot. 
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <h2 id="skeleton-search" class="title is-3">Efficiently Searching over Plan Skeletons</h2>
                <p>
                    To efficiently guide the discrete search over plan skeletons and avoid optimizing unsolvable ones, we exploit parallelized constraint sampling from particle initialization to derive a <b>plan feasibility heuristic</b>.
                    cuTAMP prioritizes skeletons for refinement based on their estimated feasibility, focusing our computational effort on those most likely to admit solutions.
                </p>
                <p>
                    In Stick Button, the robot must press the red, green, and blue buttons using either its fingers or a stick as a tool.
                    Due to the kinematic limitations of the Franka, the robot must use the stick to press the blue and green buttons, as they are out of direct reach.
                    This results in a large number of plan skeletons that are infeasible or have extraneous actions.
                    The best configuration of cuTAMP solves this problem in under 1.5 seconds.
                </p>
                <div class="columns is-multiline">
                    <div class="column is-half">
                        <div class="video-card">
                            <img src="assets/stick-button-rollout.png" alt="Stick Button Rollout" data-dark-src="assets/stick-button-rollout-dark.png" data-light-src="assets/stick-button-rollout.png">
                            <div class="video-card-content">
                                <div class="video-title">Cross-Embodiment Generalization</div>
                                <p>
                                    The blue button is beyond the reach of the Franka, requiring it to use the stick as a tool.
                                    In contrast, the UR5 can directly reach the button.
                                </p>
                            </div>
                        </div>
                    </div>
                    <div class="column">
                        <div class="video-card">
                            <video autoplay controls loop muted playsinline alt="Video">
                                <source src="assets/videos/rummy-obstruction.mp4" type="video/mp4">
                            </video>
                            <div class="video-card-content">
                                <div class="video-title">Stack Red Block üü• on Blue Block üü¶</div>
                                <p>
                                    The Mustard Bottle and Canister are obstructing the placement.
                                    cuTAMP reasons to move these objects out of the way first, before stacking the blocks.
                                </p>
                            </div>
                        </div>
                    </div>
                    <div class="column is-full">
                        <div class="video-card">
                            <video autoplay controls loop muted playsinline alt="Video">
                                <source src="assets/videos/ur5-fruit-packing.mp4" type="video/mp4">
                            </video>
                            <div class="video-card-content">
                                <div class="video-title">Pack the Fruit on the White Cube</div>
                                <p>
                                    The strawberry üçì is obstructed by Lego blocks on all four sides, requiring at least two blocks to be moved out of the way.
                                    This task requires more planning time as there are many more skeletons to be considered (there are 5698 skeletons in total).
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Citation section -->
                <h3 class="title is-4">Citation</h3>
                <div class="is-relative">
                    <pre class="textarea is-family-code" readonly id="citation-text">@inproceedings{athalye2025pix2pred,
title={From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models},
author={Athalye, Ashay and Kumar, Nishanth and Silver, Tom and Liang, Yichao and Wang, Jiuguang and Lozano-P{\'e}rez, Tom{\'a}s and Kaelbling, Leslie Pack},
booktitle={Proceedings of the Conference on Robot Learning (CoRL)},
year={2025},
note={To appear}}</pre>
                    <button id="citation-copy-button" class="button is-small" onclick="copyCitation()">
                        <span class="icon">
                            <i class="fas fa-copy"></i>
                        </span>
                    </button>
                    <div id="toast" class="notification is-success is-light is-hidden">
                        Copied to clipboard!
                    </div>
                </div>

                <hr class="divider">
                <div class="level">
                    <div class="level-left">
                        <div class="level-item">
                            <a href="https://github.com/cutamp/cutamp.github.io" class="has-text-weight-light">
                                <span class="icon"><i class="fab fa-github"></i></span> Website template from cuTAMP.
                            </a>
                        </div>
                    </div>
                    <div class="level-right">
                        <div class="level-item">
                            <p class="has-text-weight-light">Updated April 2025</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <script>
        // Handle copying citation text
        const copyCitation = async () => {
            try {
                const citationText = document.getElementById('citation-text').innerText;
                await navigator.clipboard.writeText(citationText);
                
                const toast = document.getElementById('toast');
                toast.classList.remove('is-hidden');
                setTimeout(() => toast.classList.add('is-hidden'), 2000);
            } catch (err) {
                console.error('Failed to copy citation:', err);
            }
        };

        // Handle dark/light theme toggling
        const updateTheme = (isDark) => {
            const html = document.documentElement;
            const themeIcon = document.getElementById('theme-icon');
            
            // Update theme
            html.setAttribute('data-theme', isDark ? 'dark' : 'light');
            
            // Update icon
            themeIcon.classList.remove('fa-moon', 'fa-sun');
            themeIcon.classList.add(isDark ? 'fa-sun' : 'fa-moon');
            
            // Update images
            document.querySelectorAll('img[data-dark-src]').forEach(img => {
                const desiredSrc = img.getAttribute(isDark ? 'data-dark-src' : 'data-light-src');
                if (!img.src.includes(desiredSrc)) {
                    img.src = desiredSrc;
                } else {
                    // Force reload for Safari
                    img.src = desiredSrc + '?v=' + Date.now();
                }
            });
        };

        const toggleDarkMode = () => {
            const isDark = document.documentElement.getAttribute('data-theme') === 'dark';
            updateTheme(!isDark);
        };

        // Listen for system theme changes
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e => {
            updateTheme(e.matches);
        });
    </script>
</body>
  
</html>
